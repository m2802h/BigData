{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed355ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: feedparser in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.0.12)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from feedparser) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\martin\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\martin\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans==4.0.0-rc1 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.11.12)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: influxdb-client in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.49.0)\n",
      "Requirement already satisfied: reactivex>=4.0.4 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from influxdb-client) (4.1.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from influxdb-client) (2025.11.12)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\martin\\appdata\\roaming\\python\\python311\\site-packages (from influxdb-client) (2.9.0.post0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from influxdb-client) (65.5.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from influxdb-client) (2.6.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\martin\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.5.3->influxdb-client) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.1 in c:\\users\\martin\\appdata\\roaming\\python\\python311\\site-packages (from reactivex>=4.0.4->influxdb-client) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests feedparser\n",
    "%pip install pandas\n",
    "%pip install googletrans==4.0.0-rc1\n",
    "%pip install influxdb-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ce5977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - imports + config (DB-only)\n",
    "import os\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# InfluxDB (read from env if you want)\n",
    "INFLUX_URL = os.getenv(\"INFLUX_URL\", \"http://localhost:8086\")\n",
    "INFLUX_TOKEN = os.getenv(\"INFLUX_TOKEN\", \"\")\n",
    "INFLUX_ORG = os.getenv(\"INFLUX_ORG\", \"\")\n",
    "INFLUX_BUCKET = os.getenv(\"INFLUX_BUCKET\", \"\")\n",
    "\n",
    "FEED_URL = \"https://rss.orf.at/news.xml\"\n",
    "\n",
    "TARGET_OEWA = \"urn:oewa:RedCont:Politik/PolitikAusland\"\n",
    "USER_AGENT = \"orf-rss-tracker/1.0 (+local notebook)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e6fb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14846,\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<rdf:RDF\\n  xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\\n  xmlns:dc=\"http://purl.org/dc/elements/1.1/\"\\n  xmlns:sy=\"http://purl.org/rss/1.0/modules/synd')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 - fetch XML\n",
    "def fetch_feed_xml(url: str, timeout: int = 20) -> str:\n",
    "    r = requests.get(url, timeout=timeout, headers={\"User-Agent\": USER_AGENT})\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "xml_text = fetch_feed_xml(FEED_URL)\n",
    "len(xml_text), xml_text[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e60c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
       " 'dc': 'http://purl.org/dc/elements/1.1/',\n",
       " 'sy': 'http://purl.org/rss/1.0/modules/syndication/',\n",
       " 'orfon': 'http://rss.orf.at/1.0/',\n",
       " '': 'http://purl.org/rss/1.0/'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3 - detect namespaces robustly (so you don't have to guess)\n",
    "import io\n",
    "\n",
    "def detect_namespaces(xml_text: str) -> dict:\n",
    "    ns = {}\n",
    "    for event, elem in ET.iterparse(io.StringIO(xml_text), events=(\"start-ns\",)):\n",
    "        prefix, uri = elem\n",
    "        ns[prefix if prefix is not None else \"\"] = uri\n",
    "    return ns\n",
    "\n",
    "NS = detect_namespaces(xml_text)\n",
    "NS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78b7f758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('{http://www.w3.org/1999/02/22-rdf-syntax-ns#}RDF', 24)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4 - parse + sanity checks (THIS will show why your old code returned 0)\n",
    "root = ET.fromstring(xml_text)\n",
    "\n",
    "rss_ns = NS.get(\"rss\", \"http://purl.org/rss/1.0/\")  # ORF uses RSS 1.0\n",
    "items = root.findall(\".//{%s}item\" % rss_ns)\n",
    "\n",
    "root.tag, len(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac45eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5 - helper: load already-seen usids (dedupe) from InfluxDB (DB-only)\n",
    "from influx_io import get_client, INFLUX_BUCKET, INFLUX_ORG\n",
    "\n",
    "def load_seen_usids_from_influx(lookback: str = \"30d\") -> set[str]:\n",
    "    \"\"\"\n",
    "    Pull distinct usid tag values stored in measurement 'orf_article'\n",
    "    within a recent lookback window.\n",
    "    \"\"\"\n",
    "    with get_client() as client:\n",
    "        query_api = client.query_api()\n",
    "\n",
    "        flux = f'''\n",
    "from(bucket: \"{INFLUX_BUCKET}\")\n",
    "  |> range(start: -{lookback})\n",
    "  |> filter(fn: (r) => r._measurement == \"orf_article\")\n",
    "  |> keep(columns: [\"usid\"])\n",
    "  |> distinct(column: \"usid\")\n",
    "'''\n",
    "\n",
    "        tables = query_api.query(flux, org=INFLUX_ORG)\n",
    "\n",
    "    seen = set()\n",
    "    for table in tables:\n",
    "        for record in table.records:\n",
    "            val = record.values.get(\"usid\")\n",
    "            if val:\n",
    "                seen.add(str(val))\n",
    "    return seen\n",
    "\n",
    "seen_usids = load_seen_usids_from_influx(\"30d\")\n",
    "len(seen_usids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2917efb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " [{'usid': 'news:3416944',\n",
       "   'date': '2026-01-12T18:16:58+01:00',\n",
       "   'link': 'https://orf.at/stories/3416944/',\n",
       "   'title': 'Fünf Jahre Haft für Georgiens Ex-Regierungschef',\n",
       "   'oewaCategory': 'urn:oewa:RedCont:Politik/PolitikAusland',\n",
       "   'fetched_at_utc': '2026-01-12T17:21:38.929863+00:00'},\n",
       "  {'usid': 'news:3416938',\n",
       "   'date': '2026-01-12T16:51:58+01:00',\n",
       "   'link': 'https://orf.at/stories/3416938/',\n",
       "   'title': 'Iran bestellt Botschafter mehrerer Staaten ein',\n",
       "   'oewaCategory': 'urn:oewa:RedCont:Politik/PolitikAusland',\n",
       "   'fetched_at_utc': '2026-01-12T17:21:38.929863+00:00'}])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6 - parse items + filter by oewaCategory\n",
    "def text_of(el):\n",
    "    return el.text.strip() if el is not None and el.text else None\n",
    "\n",
    "def parse_filtered_items(root: ET.Element, ns: dict) -> list[dict]:\n",
    "    rss_ns = ns.get(\"rss\", \"http://purl.org/rss/1.0/\")\n",
    "    rdf_ns = ns.get(\"rdf\", \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "    dc_ns  = ns.get(\"dc\",  \"http://purl.org/dc/elements/1.1/\")\n",
    "    orf_ns = ns.get(\"orfon\")  # must exist in feed; we'll rely on detected value\n",
    "\n",
    "    if not orf_ns:\n",
    "        raise RuntimeError(\"Could not detect 'orfon' namespace in the feed. Check NS dict output.\")\n",
    "\n",
    "    out = []\n",
    "    for item in root.findall(\".//{%s}item\" % rss_ns):\n",
    "        # orfon:oewaCategory rdf:resource=\"...\"\n",
    "        cat_el = item.find(\"{%s}oewaCategory\" % orf_ns)\n",
    "        if cat_el is None:\n",
    "            continue\n",
    "\n",
    "        cat_val = cat_el.attrib.get(\"{%s}resource\" % rdf_ns)\n",
    "        if cat_val != TARGET_OEWA:\n",
    "            continue\n",
    "\n",
    "        title_el = item.find(\"{%s}title\" % rss_ns)\n",
    "        link_el  = item.find(\"{%s}link\" % rss_ns)\n",
    "        date_el  = item.find(\"{%s}date\" % dc_ns)\n",
    "        usid_el  = item.find(\"{%s}usid\" % orf_ns)\n",
    "\n",
    "        out.append({\n",
    "            \"usid\": text_of(usid_el),\n",
    "            \"date\": text_of(date_el),\n",
    "            \"link\": text_of(link_el),\n",
    "            \"title\": text_of(title_el),\n",
    "            \"oewaCategory\": cat_val,\n",
    "            \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "filtered_items = parse_filtered_items(root, NS)\n",
    "len(filtered_items), filtered_items[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ace430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('urn:oewa:RedCont:Politik/PolitikAusland', 10),\n",
       " ('urn:oewa:RedCont:Nachrichten/Chronik', 4),\n",
       " ('urn:oewa:RedCont:Wirtschaft/Wirtschaftspolitik', 3),\n",
       " ('urn:oewa:RedCont:ComputerUndTechnik/ComputerUndTechnikUeberblick', 2),\n",
       " ('urn:oewa:RedCont:Politik/PolitikInland', 2),\n",
       " ('urn:oewa:RedCont:KulturUndFreizeit/Literatur', 1),\n",
       " ('urn:oewa:RedCont:Wirtschaft/Unternehmensberichterstattung', 1),\n",
       " ('urn:oewa:RedCont:KulturUndFreizeit/KulturUeberblick', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 7 - (optional) debug: what categories exist + counts\n",
    "from collections import Counter\n",
    "\n",
    "def category_counts(root: ET.Element, ns: dict) -> Counter:\n",
    "    rss_ns = ns.get(\"rss\", \"http://purl.org/rss/1.0/\")\n",
    "    rdf_ns = ns.get(\"rdf\", \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "    orf_ns = ns.get(\"orfon\")\n",
    "    c = Counter()\n",
    "    for item in root.findall(\".//{%s}item\" % rss_ns):\n",
    "        cat_el = item.find(\"{%s}oewaCategory\" % orf_ns) if orf_ns else None\n",
    "        if cat_el is None:\n",
    "            continue\n",
    "        cat_val = cat_el.attrib.get(\"{%s}resource\" % rdf_ns)\n",
    "        if cat_val:\n",
    "            c[cat_val] += 1\n",
    "    return c\n",
    "\n",
    "counts = category_counts(root, NS)\n",
    "counts.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac67de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote ORF articles to InfluxDB: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 8 - DB-only: keep only new items + write to InfluxDB\n",
    "from influx_io import write_orf_articles\n",
    "\n",
    "new_items = [it for it in filtered_items if it.get(\"usid\") and it[\"usid\"] not in seen_usids]\n",
    "\n",
    "written = write_orf_articles(new_items)\n",
    "print(\"Wrote ORF articles to InfluxDB:\", written)\n",
    "\n",
    "# update local seen set (so rerunning later cells in this same session doesn't re-write)\n",
    "for it in new_items:\n",
    "    seen_usids.add(it[\"usid\"])\n",
    "\n",
    "len(new_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1a26fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usid</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>oewaCategory</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news:3416944</td>\n",
       "      <td>2026-01-12T18:16:58+01:00</td>\n",
       "      <td>https://orf.at/stories/3416944/</td>\n",
       "      <td>Fünf Jahre Haft für Georgiens Ex-Regierungschef</td>\n",
       "      <td>urn:oewa:RedCont:Politik/PolitikAusland</td>\n",
       "      <td>2026-01-12T17:21:38.929863+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news:3416938</td>\n",
       "      <td>2026-01-12T16:51:58+01:00</td>\n",
       "      <td>https://orf.at/stories/3416938/</td>\n",
       "      <td>Iran bestellt Botschafter mehrerer Staaten ein</td>\n",
       "      <td>urn:oewa:RedCont:Politik/PolitikAusland</td>\n",
       "      <td>2026-01-12T17:21:38.929863+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news:3416926</td>\n",
       "      <td>2026-01-12T15:21:56+01:00</td>\n",
       "      <td>https://orf.at/stories/3416926/</td>\n",
       "      <td>116 politische Gefangene in Venezuela freigela...</td>\n",
       "      <td>urn:oewa:RedCont:Politik/PolitikAusland</td>\n",
       "      <td>2026-01-12T17:21:38.929863+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news:3416920</td>\n",
       "      <td>2026-01-12T15:16:04+01:00</td>\n",
       "      <td>https://orf.at/stories/3416920/</td>\n",
       "      <td>Pläne zu Mindestpreisen für Elektroautos aus C...</td>\n",
       "      <td>urn:oewa:RedCont:Politik/PolitikAusland</td>\n",
       "      <td>2026-01-12T17:21:38.929863+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news:3416918</td>\n",
       "      <td>2026-01-12T14:58:11+01:00</td>\n",
       "      <td>https://orf.at/stories/3416918/</td>\n",
       "      <td>Grönland-Debatte: NATO arbeitet an „nächsten S...</td>\n",
       "      <td>urn:oewa:RedCont:Politik/PolitikAusland</td>\n",
       "      <td>2026-01-12T17:21:38.929863+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>news:3416898</td>\n",
       "      <td>2026-01-12T14:13:51+01:00</td>\n",
       "      <td>https://orf.at/stories/3416898/</td>\n",
       "      <td>Was die Proteste für Regime gefährlich macht</td>\n",
       "      <td>urn:oewa:RedCont:Politik/PolitikAusland</td>\n",
       "      <td>2026-01-12T17:21:38.929863+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>news:3416902</td>\n",
       "      <td>2026-01-12T12:36:41+01:00</td>\n",
       "      <td>https://orf.at/stories/3416902/</td>\n",
       "      <td>GB will ballistische Rakete für Ukraine entwic...</td>\n",
       "      <td>urn:oewa:RedCont:Politik/PolitikAusland</td>\n",
       "      <td>2026-01-12T17:21:38.929863+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>news:3416895</td>\n",
       "      <td>2026-01-12T11:29:05+01:00</td>\n",
       "      <td>https://orf.at/stories/3416895/</td>\n",
       "      <td>Völkermordprozess gegen Myanmar in Den Haag an...</td>\n",
       "      <td>urn:oewa:RedCont:Politik/PolitikAusland</td>\n",
       "      <td>2026-01-12T17:21:38.929863+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>news:3416892</td>\n",
       "      <td>2026-01-12T11:05:25+01:00</td>\n",
       "      <td>https://orf.at/stories/3416892/</td>\n",
       "      <td>35.000 Haushalte um Odessa ohne Strom</td>\n",
       "      <td>urn:oewa:RedCont:Politik/PolitikAusland</td>\n",
       "      <td>2026-01-12T17:21:38.929863+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>news:3416889</td>\n",
       "      <td>2026-01-12T09:58:25+01:00</td>\n",
       "      <td>https://orf.at/stories/3416889/</td>\n",
       "      <td>Trump: „Wir bekommen Grönland“</td>\n",
       "      <td>urn:oewa:RedCont:Politik/PolitikAusland</td>\n",
       "      <td>2026-01-12T17:21:38.929863+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           usid                       date                             link  \\\n",
       "0  news:3416944  2026-01-12T18:16:58+01:00  https://orf.at/stories/3416944/   \n",
       "1  news:3416938  2026-01-12T16:51:58+01:00  https://orf.at/stories/3416938/   \n",
       "2  news:3416926  2026-01-12T15:21:56+01:00  https://orf.at/stories/3416926/   \n",
       "3  news:3416920  2026-01-12T15:16:04+01:00  https://orf.at/stories/3416920/   \n",
       "4  news:3416918  2026-01-12T14:58:11+01:00  https://orf.at/stories/3416918/   \n",
       "5  news:3416898  2026-01-12T14:13:51+01:00  https://orf.at/stories/3416898/   \n",
       "6  news:3416902  2026-01-12T12:36:41+01:00  https://orf.at/stories/3416902/   \n",
       "7  news:3416895  2026-01-12T11:29:05+01:00  https://orf.at/stories/3416895/   \n",
       "8  news:3416892  2026-01-12T11:05:25+01:00  https://orf.at/stories/3416892/   \n",
       "9  news:3416889  2026-01-12T09:58:25+01:00  https://orf.at/stories/3416889/   \n",
       "\n",
       "                                               title  \\\n",
       "0    Fünf Jahre Haft für Georgiens Ex-Regierungschef   \n",
       "1     Iran bestellt Botschafter mehrerer Staaten ein   \n",
       "2  116 politische Gefangene in Venezuela freigela...   \n",
       "3  Pläne zu Mindestpreisen für Elektroautos aus C...   \n",
       "4  Grönland-Debatte: NATO arbeitet an „nächsten S...   \n",
       "5       Was die Proteste für Regime gefährlich macht   \n",
       "6  GB will ballistische Rakete für Ukraine entwic...   \n",
       "7  Völkermordprozess gegen Myanmar in Den Haag an...   \n",
       "8              35.000 Haushalte um Odessa ohne Strom   \n",
       "9                     Trump: „Wir bekommen Grönland“   \n",
       "\n",
       "                              oewaCategory                    fetched_at_utc  \n",
       "0  urn:oewa:RedCont:Politik/PolitikAusland  2026-01-12T17:21:38.929863+00:00  \n",
       "1  urn:oewa:RedCont:Politik/PolitikAusland  2026-01-12T17:21:38.929863+00:00  \n",
       "2  urn:oewa:RedCont:Politik/PolitikAusland  2026-01-12T17:21:38.929863+00:00  \n",
       "3  urn:oewa:RedCont:Politik/PolitikAusland  2026-01-12T17:21:38.929863+00:00  \n",
       "4  urn:oewa:RedCont:Politik/PolitikAusland  2026-01-12T17:21:38.929863+00:00  \n",
       "5  urn:oewa:RedCont:Politik/PolitikAusland  2026-01-12T17:21:38.929863+00:00  \n",
       "6  urn:oewa:RedCont:Politik/PolitikAusland  2026-01-12T17:21:38.929863+00:00  \n",
       "7  urn:oewa:RedCont:Politik/PolitikAusland  2026-01-12T17:21:38.929863+00:00  \n",
       "8  urn:oewa:RedCont:Politik/PolitikAusland  2026-01-12T17:21:38.929863+00:00  \n",
       "9  urn:oewa:RedCont:Politik/PolitikAusland  2026-01-12T17:21:38.929863+00:00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 9 - show latest rows quickly (DB-only, from current run)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(new_items)\n",
    "df.tail(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66496a8a",
   "metadata": {},
   "source": [
    "Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd3d3e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefetched r/politics new: 800 posts\n",
      "Prefetched r/austria new: 800 posts\n",
      "Prefetched r/europe new: 800 posts\n",
      "Prefetched r/worldnews new: 800 posts\n",
      "Prefetched r/news new: 800 posts\n",
      "[1/2] news:3416944 | groups_used=7/7 | scanned_total=5702 | kept=607\n",
      "[2/2] news:3416938 | groups_used=7/7 | scanned_total=5778 | kept=515\n",
      "Wrote 0 rows to InfluxDB\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# RUN SCRIPT (full)\n",
    "# =========================\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from googletrans import Translator\n",
    "\n",
    "# df already exists:\n",
    "# df = pd.read_csv(CSV_PATH)\n",
    "df = pd.DataFrame(filtered_items).head(2)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "OUT_CSV = \"reddit_posts_minimal.csv\"\n",
    "USER_AGENT = \"orf-reddit-minimal/1.0\"\n",
    "\n",
    "TITLE_COL = \"title\"\n",
    "USID_COL = \"usid\"\n",
    "\n",
    "# --- Rate limiting ---\n",
    "REQUEST_SLEEP = 2.0      # <- normal sleep between EVERY reddit request\n",
    "BACKOFF_SLEEP = 90       # <- sleep when 429 happens\n",
    "\n",
    "# --- groups / matching ---\n",
    "MAX_KEYWORDS_DE = 20          # extract more DE keywords from title\n",
    "ADD_BIGRAM_GROUPS = True      # add phrase/bigram groups to reach ~7+ groups\n",
    "MAX_BIGRAMS = 10              # cap bigram groups per article\n",
    "\n",
    "TARGET_GROUPS = 7             # try to use 7 groups\n",
    "MIN_GROUP_MATCHES = 3         # must hit at least 3 groups\n",
    "MAX_POST_WORDS = 300          # only keep posts with <= 300 words in checked text\n",
    "\n",
    "CHECK_TEXT_MODE = \"title+selftext\"  # or \"selftext\"\n",
    "\n",
    "# --- reddit search (ranked) ---\n",
    "MAX_PAGES_PER_SOURCE = 10\n",
    "LIMIT_PER_PAGE = 100\n",
    "SORT_MODE = \"relevance\"\n",
    "\n",
    "# --- reddit subreddit feeds (/new.json) ---\n",
    "USE_SUB_NEW_FEEDS = True\n",
    "NEW_FEED_PAGES_PER_SUB = 8\n",
    "NEW_FEED_LIMIT = 100\n",
    "\n",
    "# --- where to search ---\n",
    "SUBREDDITS = [\"politics\", \"austria\", \"europe\", \"worldnews\", \"news\"]\n",
    "SEARCH_GLOBAL_TOO = True\n",
    "RESTRICT_TO_SUBS_ONLY = False\n",
    "\n",
    "# If you delete the CSV after each run, keep False\n",
    "DEDUPE_WITH_EXISTING_CSV = False\n",
    "\n",
    "# =========================\n",
    "# INIT\n",
    "# =========================\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": USER_AGENT})\n",
    "translator = Translator()\n",
    "\n",
    "STOPWORDS = {\n",
    "    # DE\n",
    "    \"der\",\"die\",\"das\",\"und\",\"oder\",\"nicht\",\"nur\",\"auch\",\"mit\",\"von\",\"für\",\"über\",\"unter\",\"nach\",\"vor\",\n",
    "    \"ein\",\"eine\",\"einer\",\"eines\",\"dem\",\"den\",\"des\",\"im\",\"in\",\"am\",\"an\",\"auf\",\"aus\",\"bei\",\"zum\",\"zur\",\n",
    "    \"ist\",\"sind\",\"war\",\"waren\",\"wird\",\"werden\",\"hat\",\"haben\",\"kann\",\"können\",\"muss\",\"müssen\",\n",
    "    # EN\n",
    "    \"the\",\"and\",\"or\",\"not\",\"only\",\"also\",\"with\",\"from\",\"for\",\"about\",\"this\",\"that\",\"these\",\"those\",\n",
    "    \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"has\",\"have\",\"had\",\"can\",\"could\",\"must\",\"will\",\"may\",\n",
    "    \"of\",\"by\",\"to\",\"in\",\"on\",\"at\"\n",
    "}\n",
    "\n",
    "WORD_RE = re.compile(r\"\\b[\\wÄÖÜäöüß]+\\b\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def safe_str(x) -> str:\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(x)\n",
    "\n",
    "\n",
    "def words_list(text: str) -> list[str]:\n",
    "    return WORD_RE.findall((text or \"\").strip())\n",
    "\n",
    "\n",
    "def tokenize_list(text: str) -> list[str]:\n",
    "    \"\"\"Ordered tokens, lowercased, stopwords removed, len>=3.\"\"\"\n",
    "    text = (text or \"\").lower()\n",
    "    text = re.sub(r\"[^0-9a-zäöüß]+\", \" \", text, flags=re.IGNORECASE)\n",
    "    toks = []\n",
    "    for t in text.split():\n",
    "        if len(t) >= 3 and t not in STOPWORDS:\n",
    "            toks.append(t)\n",
    "    return toks\n",
    "\n",
    "\n",
    "def tokenize_set(text: str) -> set[str]:\n",
    "    return set(tokenize_list(text))\n",
    "\n",
    "\n",
    "def normalize_term_variants(term: str) -> set[str]:\n",
    "    \"\"\"\n",
    "    Lightweight variants:\n",
    "    - lower\n",
    "    - naive plural/singular + DE endings trimming\n",
    "    \"\"\"\n",
    "    t = (term or \"\").strip().lower()\n",
    "    if not t:\n",
    "        return set()\n",
    "    vars_ = {t}\n",
    "\n",
    "    # EN plural -> singular heuristic\n",
    "    if t.endswith(\"s\") and len(t) > 3:\n",
    "        vars_.add(t[:-1])\n",
    "\n",
    "    # DE common endings heuristic\n",
    "    for suf in (\"en\", \"er\", \"e\", \"n\", \"s\"):\n",
    "        if t.endswith(suf) and len(t) > len(suf) + 2:\n",
    "            vars_.add(t[: -len(suf)])\n",
    "\n",
    "    return {v for v in vars_ if len(v) >= 3 and v not in STOPWORDS}\n",
    "\n",
    "\n",
    "def build_keywords_de(title: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Deterministic DE keywords:\n",
    "    - unique tokens\n",
    "    - prefer longer tokens\n",
    "    \"\"\"\n",
    "    toks = tokenize_list(title)\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for t in toks:\n",
    "        if t not in seen:\n",
    "            uniq.append(t)\n",
    "            seen.add(t)\n",
    "    uniq_sorted = sorted(uniq, key=lambda x: (-len(x), x))\n",
    "    return uniq_sorted[:MAX_KEYWORDS_DE]\n",
    "\n",
    "\n",
    "def translate_text_de_to_en(text: str) -> str | None:\n",
    "    \"\"\"Best-effort translation; returns None on failure/empty.\"\"\"\n",
    "    try:\n",
    "        t = translator.translate(text, src=\"de\", dest=\"en\").text\n",
    "        t = (t or \"\").strip().lower()\n",
    "        return t if t else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def translate_keywords_to_en(keywords_de: list[str]) -> dict[str, str]:\n",
    "    \"\"\"Mapping {de_kw: en_kw} best-effort.\"\"\"\n",
    "    m = {}\n",
    "    for kw in keywords_de:\n",
    "        t = translate_text_de_to_en(kw)\n",
    "        if t:\n",
    "            m[kw] = t\n",
    "    return m\n",
    "\n",
    "\n",
    "def build_bigram_phrases(title: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Build phrase candidates from title tokens in order.\n",
    "    \"\"\"\n",
    "    toks = tokenize_list(title)\n",
    "    bigrams = []\n",
    "    for i in range(len(toks) - 1):\n",
    "        bigrams.append(f\"{toks[i]} {toks[i+1]}\")\n",
    "    bigrams = sorted(list(dict.fromkeys(bigrams)), key=lambda x: (-len(x), x))\n",
    "    return bigrams[:MAX_BIGRAMS]\n",
    "\n",
    "\n",
    "def build_keyword_groups_from_title(title: str) -> tuple[list[set[str]], dict[str, str], list[str]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - groups (list[set[str]])  : synonym groups (unigrams + optional bigrams)\n",
    "      - de_to_en_map (dict)      : mapping for unigrams and bigrams that were translated\n",
    "      - debug_reps (list[str])   : representative term per group (for logging)\n",
    "    \"\"\"\n",
    "    keywords_de = build_keywords_de(title)\n",
    "    de_to_en = translate_keywords_to_en(keywords_de)\n",
    "\n",
    "    groups: list[set[str]] = []\n",
    "\n",
    "    # unigram groups\n",
    "    for de_kw in keywords_de:\n",
    "        g = set()\n",
    "        g |= normalize_term_variants(de_kw)\n",
    "        en_kw = de_to_en.get(de_kw)\n",
    "        if en_kw:\n",
    "            g |= normalize_term_variants(en_kw)\n",
    "        if g:\n",
    "            groups.append(g)\n",
    "\n",
    "    # bigram groups\n",
    "    if ADD_BIGRAM_GROUPS:\n",
    "        for bg_de in build_bigram_phrases(title):\n",
    "            g = set()\n",
    "            g.add(bg_de.lower())\n",
    "            for tok in tokenize_list(bg_de):\n",
    "                g |= normalize_term_variants(tok)\n",
    "\n",
    "            bg_en = translate_text_de_to_en(bg_de)\n",
    "            if bg_en:\n",
    "                de_to_en[bg_de] = bg_en\n",
    "                g.add(bg_en.lower())\n",
    "                for tok in tokenize_list(bg_en):\n",
    "                    g |= normalize_term_variants(tok)\n",
    "\n",
    "            groups.append(g)\n",
    "\n",
    "    # dedupe identical groups\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for g in groups:\n",
    "        key = tuple(sorted(g))\n",
    "        if key not in seen:\n",
    "            uniq.append(g)\n",
    "            seen.add(key)\n",
    "    groups = uniq\n",
    "\n",
    "    # sort groups by specificity (longest representative)\n",
    "    scored = []\n",
    "    for g in groups:\n",
    "        rep = sorted(g, key=lambda x: (-len(x), x))[0]\n",
    "        scored.append((len(rep), rep, g))\n",
    "    scored.sort(reverse=True)\n",
    "\n",
    "    picked = [g for _, _, g in scored[:TARGET_GROUPS]]\n",
    "    reps = [rep for _, rep, _ in scored[:TARGET_GROUPS]]\n",
    "\n",
    "    return picked, de_to_en, reps\n",
    "\n",
    "\n",
    "def group_hit_count(tokens: set[str], groups: list[set[str]]) -> int:\n",
    "    return sum(1 for g in groups if (tokens & g))\n",
    "\n",
    "\n",
    "def get_check_text(post: dict) -> str:\n",
    "    \"\"\"\n",
    "    Include URL as extra matchable text (important for link posts).\n",
    "    \"\"\"\n",
    "    title = safe_str(post.get(\"title\", \"\"))\n",
    "    selftext = safe_str(post.get(\"selftext\", \"\"))\n",
    "    url = safe_str(post.get(\"url\", \"\"))\n",
    "\n",
    "    base = selftext if CHECK_TEXT_MODE == \"selftext\" else f\"{title} {selftext}\".strip()\n",
    "    return f\"{base} {url}\".strip()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# REQUEST WRAPPER (rate limited)\n",
    "# =========================\n",
    "def reddit_get(url: str, *, params: dict | None = None, timeout: int = 30):\n",
    "    \"\"\"\n",
    "    Always sleeps REQUEST_SLEEP after each request.\n",
    "    On 429, sleeps BACKOFF_SLEEP and retries once.\n",
    "    Returns response or None if still 429/failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = session.get(url, params=params, timeout=timeout)\n",
    "        time.sleep(REQUEST_SLEEP)\n",
    "\n",
    "        if r.status_code == 429:\n",
    "            print(\"⚠️ 429 rate limit. Backing off...\")\n",
    "            time.sleep(BACKOFF_SLEEP)\n",
    "            r = session.get(url, params=params, timeout=timeout)\n",
    "            time.sleep(REQUEST_SLEEP)\n",
    "\n",
    "        if r.status_code == 429:\n",
    "            print(\"⚠️ 429 again. Skipping this request.\")\n",
    "            return None\n",
    "\n",
    "        r.raise_for_status()\n",
    "        return r\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# REDDIT FETCHERS\n",
    "# =========================\n",
    "def reddit_search(query: str, subreddit: str | None = None):\n",
    "    \"\"\"Search either globally or within a subreddit.\"\"\"\n",
    "    if subreddit:\n",
    "        base = f\"https://www.reddit.com/r/{subreddit}/search.json\"\n",
    "    else:\n",
    "        base = \"https://www.reddit.com/search.json\"\n",
    "\n",
    "    after = None\n",
    "    for _ in range(MAX_PAGES_PER_SOURCE):\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"sort\": SORT_MODE,\n",
    "            \"limit\": LIMIT_PER_PAGE,\n",
    "            \"syntax\": \"lucene\",\n",
    "        }\n",
    "        if subreddit:\n",
    "            params[\"restrict_sr\"] = 1\n",
    "        if after:\n",
    "            params[\"after\"] = after\n",
    "\n",
    "        r = reddit_get(base, params=params)\n",
    "        if r is None:\n",
    "            return\n",
    "\n",
    "        data = r.json().get(\"data\", {})\n",
    "        children = data.get(\"children\", [])\n",
    "        if not children:\n",
    "            return\n",
    "\n",
    "        for ch in children:\n",
    "            yield ch.get(\"data\", {})\n",
    "\n",
    "        after = data.get(\"after\")\n",
    "        if not after:\n",
    "            return\n",
    "\n",
    "\n",
    "def iter_posts_for_query(query: str):\n",
    "    \"\"\"Global + subreddit search; dedupe across sources by reddit_id.\"\"\"\n",
    "    seen = set()\n",
    "\n",
    "    if SEARCH_GLOBAL_TOO and not RESTRICT_TO_SUBS_ONLY:\n",
    "        for p in reddit_search(query, subreddit=None):\n",
    "            rid = safe_str(p.get(\"id\"))\n",
    "            if rid and rid not in seen:\n",
    "                seen.add(rid)\n",
    "                yield p\n",
    "\n",
    "    for sub in SUBREDDITS:\n",
    "        for p in reddit_search(query, subreddit=sub):\n",
    "            rid = safe_str(p.get(\"id\"))\n",
    "            if rid and rid not in seen:\n",
    "                seen.add(rid)\n",
    "                yield p\n",
    "\n",
    "\n",
    "def subreddit_new_feed(subreddit: str, pages: int = NEW_FEED_PAGES_PER_SUB, limit: int = NEW_FEED_LIMIT):\n",
    "    \"\"\"Iterate newest posts from a subreddit feed (bypasses search limits).\"\"\"\n",
    "    base = f\"https://www.reddit.com/r/{subreddit}/new.json\"\n",
    "    after = None\n",
    "\n",
    "    for _ in range(pages):\n",
    "        params = {\"limit\": limit}\n",
    "        if after:\n",
    "            params[\"after\"] = after\n",
    "\n",
    "        r = reddit_get(base, params=params)\n",
    "        if r is None:\n",
    "            return\n",
    "\n",
    "        data = r.json().get(\"data\", {})\n",
    "        children = data.get(\"children\", [])\n",
    "        if not children:\n",
    "            return\n",
    "\n",
    "        for ch in children:\n",
    "            yield ch.get(\"data\", {})\n",
    "\n",
    "        after = data.get(\"after\")\n",
    "        if not after:\n",
    "            return\n",
    "\n",
    "\n",
    "def prefetch_new_feeds(subreddits: list[str]) -> dict[str, list[dict]]:\n",
    "    \"\"\"Prefetch new posts once per run so each article can filter locally.\"\"\"\n",
    "    cache: dict[str, list[dict]] = {}\n",
    "    for sub in subreddits:\n",
    "        posts = []\n",
    "        seen = set()\n",
    "        try:\n",
    "            for p in subreddit_new_feed(sub):\n",
    "                rid = safe_str(p.get(\"id\"))\n",
    "                if rid and rid not in seen:\n",
    "                    seen.add(rid)\n",
    "                    posts.append(p)\n",
    "        except Exception as e:\n",
    "            print(f\"Prefetch error for r/{sub}: {e}\")\n",
    "        cache[sub] = posts\n",
    "        print(f\"Prefetched r/{sub} new: {len(posts)} posts\")\n",
    "    return cache\n",
    "\n",
    "\n",
    "# =========================\n",
    "# OPTIONAL DEDUPE WITH EXISTING CSV\n",
    "# =========================\n",
    "existing_pairs = set()\n",
    "if DEDUPE_WITH_EXISTING_CSV and os.path.exists(OUT_CSV):\n",
    "    existing = pd.read_csv(OUT_CSV, encoding=\"utf-8\")\n",
    "    if {\"article_usid\", \"reddit_id\"}.issubset(existing.columns):\n",
    "        existing_pairs = set(zip(existing[\"article_usid\"].astype(str), existing[\"reddit_id\"].astype(str)))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PREFETCH /new.json POSTS\n",
    "# =========================\n",
    "new_feed_cache = {}\n",
    "if USE_SUB_NEW_FEEDS:\n",
    "    new_feed_cache = prefetch_new_feeds(SUBREDDITS)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "rows = []\n",
    "\n",
    "for idx, r in df.iterrows():\n",
    "    article_usid = safe_str(r.get(USID_COL))\n",
    "    article_title = safe_str(r.get(TITLE_COL))\n",
    "    if not article_usid or not article_title:\n",
    "        continue\n",
    "\n",
    "    groups, de_to_en_map, group_reps = build_keyword_groups_from_title(article_title)\n",
    "    if len(groups) < MIN_GROUP_MATCHES:\n",
    "        print(f\"[{idx+1}/{len(df)}] {article_usid} skipped (only {len(groups)} groups)\")\n",
    "        continue\n",
    "\n",
    "    # Build DE & EN queries\n",
    "    reps = group_reps[:]\n",
    "    reps_de = [t for t in reps if (re.search(r\"[äöüß]\", t) or t in tokenize_set(article_title))]\n",
    "    if len(reps_de) < 3:\n",
    "        reps_de = reps[:]\n",
    "    reps_de = reps_de[:TARGET_GROUPS]\n",
    "\n",
    "    en_candidates = sorted({v for v in de_to_en_map.values() if v}, key=lambda x: (-len(x), x))\n",
    "    reps_en = en_candidates[:TARGET_GROUPS] if len(en_candidates) >= 3 else reps[:TARGET_GROUPS]\n",
    "\n",
    "    query_de = \"(\" + \" OR \".join(f\"\\\"{k}\\\"\" for k in reps_de) + \")\"\n",
    "    query_en = \"(\" + \" OR \".join(f\"\\\"{k}\\\"\" for k in reps_en) + \")\"\n",
    "\n",
    "    candidates: dict[str, tuple[dict, str]] = {}  # rid -> (post, source)\n",
    "\n",
    "    def add_candidates_from_iter(it, source: str):\n",
    "        for p in it:\n",
    "            rid = safe_str(p.get(\"id\"))\n",
    "            if not rid:\n",
    "                continue\n",
    "            if rid not in candidates:\n",
    "                candidates[rid] = (p, source)\n",
    "\n",
    "    # searches\n",
    "    add_candidates_from_iter(iter_posts_for_query(query_de), \"search_de\")\n",
    "    add_candidates_from_iter(iter_posts_for_query(query_en), \"search_en\")\n",
    "\n",
    "    # new feeds\n",
    "    if USE_SUB_NEW_FEEDS:\n",
    "        for sub, posts in new_feed_cache.items():\n",
    "            for p in posts:\n",
    "                rid = safe_str(p.get(\"id\"))\n",
    "                if rid and rid not in candidates:\n",
    "                    candidates[rid] = (p, f\"new_{sub}\")\n",
    "\n",
    "    scanned_total = len(candidates)\n",
    "    kept = 0\n",
    "\n",
    "    for rid, (post, source) in candidates.items():\n",
    "        if DEDUPE_WITH_EXISTING_CSV and (article_usid, rid) in existing_pairs:\n",
    "            continue\n",
    "\n",
    "        text = get_check_text(post)\n",
    "        words = words_list(text)\n",
    "        if len(words) > MAX_POST_WORDS:\n",
    "            continue\n",
    "\n",
    "        window_text = \" \".join(words[:MAX_POST_WORDS])\n",
    "        tokens = tokenize_set(window_text)\n",
    "\n",
    "        matches = group_hit_count(tokens, groups)\n",
    "        if matches < MIN_GROUP_MATCHES:\n",
    "            continue\n",
    "\n",
    "        matched_reps = []\n",
    "        for g, rep in zip(groups, group_reps):\n",
    "            if tokens & g:\n",
    "                matched_reps.append(rep)\n",
    "\n",
    "        rows.append({\n",
    "            \"article_usid\": article_usid,\n",
    "            \"reddit_id\": rid,\n",
    "            \"reddit_title\": safe_str(post.get(\"title\", \"\")),\n",
    "            \"reddit_selftext\": safe_str(post.get(\"selftext\", \"\")),\n",
    "            \"post_url\": safe_str(post.get(\"url\", \"\")),\n",
    "            \"reddit_permalink\": \"https://www.reddit.com\" + safe_str(post.get(\"permalink\", \"\")),\n",
    "            \"source\": source,\n",
    "            \"checked_word_count\": len(words),\n",
    "            \"groups_used\": len(groups),\n",
    "            \"group_matches_in_window\": matches,\n",
    "            \"matched_group_reps\": \",\".join(matched_reps),\n",
    "            \"query_de\": query_de,\n",
    "            \"query_en\": query_en,\n",
    "            \"saved_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "        })\n",
    "        kept += 1\n",
    "\n",
    "    print(\n",
    "        f\"[{idx+1}/{len(df)}] {article_usid} | \"\n",
    "        f\"groups_used={len(groups)}/{TARGET_GROUPS} | \"\n",
    "        f\"scanned_total={scanned_total} | kept={kept}\"\n",
    "    )\n",
    "\n",
    "# Write CSV\n",
    "# Write to InfluxDB instead of CSV (DB-only)\n",
    "from influx_io import write_orf_articles\n",
    "\n",
    "if rows:\n",
    "    written = write_orf_articles(rows)\n",
    "    print(f\"Wrote {written} rows to InfluxDB\")\n",
    "else:\n",
    "    print(\"No matching posts found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9bc14f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Reddit matches to InfluxDB: 1122\n"
     ]
    }
   ],
   "source": [
    "from influx_io import write_reddit_matches\n",
    "\n",
    "written = write_reddit_matches(rows)\n",
    "print(\"Wrote Reddit matches to InfluxDB:\", written)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e6a670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit row keys: dict_keys(['article_usid', 'reddit_id', 'reddit_title', 'reddit_selftext', 'post_url', 'reddit_permalink', 'source', 'checked_word_count', 'groups_used', 'group_matches_in_window', 'matched_group_reps', 'query_de', 'query_en', 'saved_at_utc'])\n",
      "Wrote Reddit matches to InfluxDB: 1122\n"
     ]
    }
   ],
   "source": [
    "# --- Reddit write (CORRECT PLACE) ---\n",
    "from influx_io import write_reddit_matches\n",
    "\n",
    "reddit_rows = rows\n",
    "\n",
    "# sanity check (optional)\n",
    "if reddit_rows:\n",
    "    print(\"Reddit row keys:\", reddit_rows[0].keys())\n",
    "\n",
    "written_reddit = write_reddit_matches(reddit_rows)\n",
    "print(\"Wrote Reddit matches to InfluxDB:\", written_reddit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
